# ================================================================
# config_hybrid_unified_v12c_high_entropy.yaml
# エントロピー係数を大幅増加（探索促進）
# 
# 【検証目的】
# PPOが早期に「直近隊100%」という局所解に収束している
# 可能性を検証。エントロピーを高くして探索を促進すると、
# 他の戦略を発見できるか？
# 
# 【期待される結果】
# - 直近隊100% → 探索不足が原因ではない
# - 中間的な値 → 探索促進が有効
# - ランダムに近い → エントロピーが高すぎる
# ================================================================

inherits: ./config.yaml

experiment:
  name: "hybrid_unified_v12c_high_entropy"
  description: |
    エントロピー係数を0.035→0.15に増加
    探索不足で局所解に陥っているか検証
  seed: 2025
  device: "cuda"

ppo:
  n_episodes: 500
  batch_size: 512
  n_epochs: 4
  clip_epsilon: 0.1
  learning_rate:
    actor: 0.0003
    critic: 0.001
    scheduler: "constant"
  gamma: 0.99
  gae_lambda: 0.95
  entropy_coef: 0.15            # ★★★ 0.035→0.15に増加 ★★★
  max_grad_norm: 0.5

state_encoding:
  mode: 'compact'
  top_k: 10
  normalization:
    max_travel_time_minutes: 30
    max_station_distance_km: 10
  coverage_aware_sorting:
    enabled: true
    time_weight: 0.6
    coverage_weight: 0.4
    sample_size: 61
    sample_radius: 4
    pre_filter_size: 30
    use_adaptive_sampling: true
    max_candidates_for_advanced: 50
  compact_coverage:
    sample_radius: 4
    sample_size: 61

hybrid_mode:
  enabled: true
  severity_classification:
    severe_conditions: ["重症", "重篤", "死亡"]
    mild_conditions: ["軽症", "中等症"]

reward:
  system:
    dispatch_failure: -1.0
    no_available_ambulance: 0.0
  unified:
    score_based_mode: true        # スコアベース報酬を使用
    score_scale: 10.0
    time_score_weight: 0.6
    coverage_loss_weight: 0.4
    time_normalization: 13.0
    coverage_w6: 0.5
    coverage_w13: 0.5
    critical_max_bonus: 50.0
    critical_lambda: 0.115
    critical_penalty_scale: 5.0
    critical_penalty_power: 1.5
    mild_max_bonus: 10.0
    mild_penalty_scale: 1.0
    coverage_penalty_scale: 35.0
    time_weight: 0.6
    coverage_weight: 0.4
  core:
    mild_params:
      sample_points: 61
      sample_radius: 4
      coverage_6min_weight: 0.5
      coverage_13min_weight: 0.5

network:
  actor:
    hidden_layers: [128, 64]
    activation: "relu"
    dropout: 0.2
  critic:
    hidden_layers: [128, 64]
    activation: "relu"
    dropout: 0.1

training:
  checkpoint_interval: 500
  keep_last_n: 7
  early_stopping:
    enabled: true
    patience: 500
    min_delta: 0.001
  logging:
    interval: 50
    tensorboard: false
    wandb: true
    wandb_project: "ems_ppo_hybrid"

evaluation:
  interval: 500
  n_eval_episodes: 10
  compare_baselines: ["closest"]
  metrics:
    - "critical_6min_rate"
    - "achieved_13min_rate"
    - "mean_response_time"
    - "non_closest_selection_rate"

data:
  data_paths:
    grid_mapping: "data/tokyo/processed/grid_mapping_res9.json"
    travel_time_matrix: "data/tokyo/calibration2/linear_calibrated_response.npy"
  episode_duration_hours: 24
  exclude_daytime_ambulances: true
  area_restriction:
    enabled: false
  train_periods:
    - start_date: "20230401"
      end_date: "20230430"
  eval_periods:
    - start_date: "20230501"
      end_date: "20230507"

severity:
  categories:
    critical:
      conditions: ["重症", "重篤", "死亡"]
      reward_weight: 1.0
    mild:
      conditions: ["軽症", "中等症"]
      reward_weight: 1.0
