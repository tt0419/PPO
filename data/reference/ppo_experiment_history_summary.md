# ================================================================
# PPO救急車配車最適化 実験履歴・知見まとめ
# 作成日: 2026年1月23日
# ================================================================

## 1. 研究の背景と目標

### 1.1 研究目的
東京23区の救急車配車システムにおいて、PPO（Proximal Policy Optimization）を用いた
強化学習により、傷病度考慮運用と同等以上の配車戦略を学習することを目指す。

### 1.2 成功基準
| 指標 | 目標値 | 傷病度考慮運用参考値 |
|------|--------|---------------------|
| 重症RT | < 7分 | 6.8分（閑散期） |
| 全体RT | < 9分 | 8.8分（閑散期） |
| 直近隊選択率 | 40-60% | 40-60% |

### 1.3 ベースライン戦略
| 戦略 | 概要 | 全体RT（繁忙期） |
|------|------|------------------|
| 直近隊運用 | 最も近い救急隊を配車 | 11.4分 |
| 傷病度考慮運用 | 重症は直近隊、軽症はカバレッジ考慮 | 12.3分 |
| MEXCLP | カバレッジ最大化ベースの配置最適化 | 13.0分 |

---

## 2. 実験履歴

### Phase 1: 初期実験（v2-v3系）

#### hybrid_balanced_v2
- **設定**: time_weight=0.5, coverage_weight=0.5, penalty_scale=20
- **結果**: 直近隊選択率100%、全体RT 7.6-11.2分
- **分析**: カバレッジを全く考慮しない「直近隊運用の模倣」

#### hybrid_time_focused_v3
- **設定**: time_weight=0.6, coverage_weight=0.4, penalty_scale=15
- **結果**: v2とほぼ同一（直近隊選択率100%）
- **分析**: 報酬の重み調整だけでは効果なし

### Phase 2: カバレッジ統一実験（v7系）

#### 根本問題の発見
v7系の設計時に、**カバレッジ計算のリング数が3箇所で不整合**であることを発見。

| 箇所 | リング数 | 使用場面 |
|------|---------|---------|
| CompactStateEncoder | ハードコード: 2 | 状態空間のL6, L13計算 |
| ems_environment | config参照: 4 | 報酬計算のL6, L13 |
| dispatch_strategies | ハードコード: 6 | ベースライン比較 |

#### hybrid_unified_v7_time（quick: 800ep）
- **設定**: time_weight=0.6, coverage_weight=0.4, penalty_scale=35
- **結果**: 直近隊選択率20-27%、全体RT 11.8-16.9分
- **分析**: カバレッジを考慮するが、過度に重視して全体RTが悪化

#### hybrid_unified_v7_time（3000ep）
- **結果**: quickより悪化（直近隊選択率11-18%、全体RT 13.3-18.2分）
- **重要発見**: **長期学習でカバレッジ最小化に収束する問題**

#### hybrid_unified_v7_lowpen
- **設定**: coverage_weight=0.7, penalty_scale=15
- **結果**: v7_timeとほぼ同等
- **分析**: カバレッジ重みを変えても挙動は変わらない

### Phase 3: リング数統一実験（v8系）

v7系の問題を受けて、**全箇所でリング数を統一**する方針に変更。

#### v8（リング6統一）
- **設定**: sample_radius=6, penalty_scale=35
- **結果**: 直近隊選択率**100%**、全体RT 7.3-11.2分
- **分析**: リング6ではカバレッジ損失の差が微小で、PPOが区別できない

#### v8b（リング6 + ペナルティ正規化）
- **設定**: coverage_penalty_scale=1.0
- **結果**: v8と完全同一
- **分析**: ペナルティスケールの変更だけでは効果なし

#### v8d（リング2統一）
- **設定**: sample_radius=2, sample_size=19
- **結果**: 直近隊選択率9-17%、全体RT 10.5-16.3分
- **分析**: リング2ではカバレッジ損失の差が大きすぎて、過度に重視

#### v8d2（リング2 + ペナルティ10）
- **設定**: coverage_penalty_scale=10.0
- **結果**: 直近隊選択率7-25%、全体RT 10.5-15.8分
- **分析**: v8dより微改善だが、根本解決には至らず

#### v8g（リング4統一）
- **設定**: sample_radius=4, sample_size=61
- **結果**: 直近隊選択率10-18%、全体RT 9.2-14.5分
- **分析**: リング2とリング6の中間だが、まだカバレッジ過重視

---

## 3. 設定パラメータ一覧

### 3.1 リング数とセル数の関係
| リング数 | セル数 | 特性 |
|----------|--------|------|
| 2 | 19 | 局所的、カバレッジ損失の差が大きい |
| 4 | 61 | 中間的 |
| 6 | 127 | 広域的、カバレッジ損失の差が小さい |

### 3.2 各バージョンの設定比較
| バージョン | リング | penalty_scale | time_weight | 直近隊率 | 全体RT |
|------------|--------|---------------|-------------|---------|--------|
| v8 | 6 | 35 | 0.6 | 100% | 7-11分 |
| v8b | 6 | 1 | 0.6 | 100% | 7-11分 |
| v8d | 2 | 35 | 0.6 | 9-17% | 10-16分 |
| v8d2 | 2 | 10 | 0.6 | 7-25% | 10-16分 |
| v8g | 4 | 35 | 0.6 | 10-18% | 9-15分 |
| **v8h** | 4 | 35 | **0.8** | ? | ? |

---

## 4. 得られた知見

### 4.1 PPO学習の特性

#### 知見1: 二極化問題
PPOは「時間最優先（直近隊100%）」か「カバレッジ最優先（直近隊10%）」の
どちらかに収束しやすく、**中間的なバランス（40-60%）を学習することが困難**。

#### 知見2: 長期学習の罠
- 800epでは比較的バランスが取れた状態
- 3000epでは「カバレッジ損失最小化」に過度に収束
- **quickテストで良い結果でも、長期学習で悪化する可能性**

#### 知見3: パラメータ感度
- **リング数**: 最も影響が大きい。リング2→6で直近隊選択率が9%→100%に変化
- **penalty_scale**: 影響が小さい。35→1でも挙動はほぼ変わらず
- **time_weight/coverage_weight**: 中程度の影響

### 4.2 報酬設計の課題

#### 課題1: カバレッジペナルティに上限がない
現在の報酬構造では、カバレッジ損失を際限なく最小化する方向に学習が進む。

#### 課題2: 報酬スケールの不均衡
```
time_reward: 0〜10（明確な勾配）
coverage_penalty: 0〜-35（上限なし）
```
カバレッジペナルティが時間報酬を圧倒する可能性。

#### 課題3: 傷病度考慮運用との違い
- **傷病度考慮運用**: 各ステップで「スコア最小」の救急隊を貪欲に選択
- **PPO**: 「期待報酬最大化」を学習
同じパラメータでも、根本的に異なるアルゴリズム。

### 4.3 設定整合性の重要性

#### 発見: リング数不整合問題
状態空間・報酬・ベースラインで異なるリング数を使用していると、
学習が破綻する。**全箇所で同一のパラメータを使用することが必須**。

---

## 5. v8h結果と分析

### 5.1 v8h実験結果（time_weight=0.8）
| 期間 | v8h 全体RT | v8h 直近隊率 | v8g 全体RT | v8g 直近隊率 |
|------|-----------|-------------|-----------|-------------|
| 2/4 | 9.83 | 12.1% | 9.90 | 12.0% |
| 3/31 | 9.34 | 11.1% | 9.23 | 11.1% |
| 5/5 | 9.24 | 9.8% | 9.18 | 10.2% |
| 6/30 | 12.27 | 11.8% | 12.25 | 12.1% |
| 7/21 | 14.05 | 14.8% | 13.91 | 14.7% |
| 12/22 | 14.42 | 17.8% | 14.47 | 17.8% |

### 5.2 重要な発見
**time_weightを0.6→0.8に変更しても、結果がほぼ同一。**

これは報酬の重み変更だけでは解決しないことを示す。
PPOは状態空間のカバレッジ情報（L6, L13）を直接見て判断しており、
報酬の重みを変えても選択行動は変わらない。

### 5.3 根本原因の再分析
```
状態空間: [移動時間, L6, L13, ...] × 10台
     ↓
PPOは「L6, L13が小さい救急隊」を直接選択
     ↓
報酬の重みを変えても、状態空間の構造が変わらない
     ↓
カバレッジペナルティを際限なく最小化する方向に収束
```

---

## 6. 今後の実験計画（v9シリーズ）

### 6.1 v9シリーズ: カバレッジペナルティ上限設定

**アプローチ**: カバレッジペナルティに上限を設けることで、
際限ないカバレッジ最小化を防ぐ。

```python
# 現在
coverage_penalty = -35 * combined_loss  # 範囲: 0 ~ -35

# v9シリーズ
raw_penalty = 35 * combined_loss
capped_penalty = min(raw_penalty, penalty_max)  # 上限でクリップ
coverage_penalty = -capped_penalty
```

### 6.2 v9バリエーション
| 実験名 | coverage_penalty_max | 期待される直近隊率 |
|--------|---------------------|-------------------|
| v9a | 3.0 | 60-80%（時間重視寄り） |
| v9b | 5.0 | 40-60%（バランス・目標値） |
| v9c | 7.0 | 20-40%（カバレッジ重視寄り） |

### 6.3 実装修正箇所
```python
# reward_designer.py の _calculate_coverage_reward メソッド
def _calculate_coverage_reward(self, L6: float, L13: float) -> float:
    p = self.coverage_params
    combined_loss = p['w6'] * L6 + p['w13'] * L13
    raw_penalty = p['penalty_scale'] * combined_loss
    
    # ★★★ 新機能: ペナルティ上限でクリップ ★★★
    penalty_max = p.get('penalty_max', float('inf'))
    capped_penalty = min(raw_penalty, penalty_max)
    
    return -capped_penalty
```

### 6.4 期待される効果
- 上限に達したら、それ以上カバレッジを最小化しても報酬は変わらない
- 時間報酬（0〜10）との勝負になる
- 直近隊選択率が上昇する見込み

---

## 6. 実験結果詳細データ

### 6.1 季節・象限別の結果比較（主要バージョン）

#### 冬・低件数×高重症（2/4-2/10）
| 戦略 | 重症RT | 全体RT | 直近隊率 |
|------|--------|--------|---------|
| 直近隊運用 | 8.0 | 8.0 | - |
| 傷病度考慮 | 7.6 | 9.0 | 40-60% |
| v8 | 8.0 | 7.9 | 100% |
| v8d | 8.2 | 11.8 | 9.4% |
| v8g | 7.9 | 9.9 | 12.0% |

#### 冬・高件数×高重症（12/22-12/28）
| 戦略 | 重症RT | 全体RT | 直近隊率 |
|------|--------|--------|---------|
| 直近隊運用 | 11.1 | 11.4 | - |
| 傷病度考慮 | 10.8 | 12.3 | 40-60% |
| v8 | 10.8 | 11.2 | 100% |
| v8d | 11.3 | 16.3 | 17.2% |
| v8g | 11.0 | 14.5 | 17.8% |

---

## 7. ファイル構成

### 7.1 configファイル
```
config_hybrid_unified_v8.yaml      # リング6統一
config_hybrid_unified_v8b.yaml     # リング6 + ペナルティ正規化
config_hybrid_unified_v8d_ring2.yaml   # リング2統一
config_hybrid_unified_v8g_ring4.yaml   # リング4統一
config_hybrid_unified_v8h_time_focused.yaml  # リング4 + time_weight強化
```

### 7.2 関連コード（要修正箇所）
```
reinforcement_learning/environment/state_encoder.py
  - CompactStateEncoder: coverage_sample_radiusをconfigから読み込むように修正済み

reinforcement_learning/environment/ems_environment.py
  - 報酬計算のカバレッジパラメータ参照

dispatch_strategies.py
  - 傷病度考慮運用の実装（リング6ハードコード）
```

---

## 8. 結論と推奨事項

### 8.1 現時点での結論
1. PPOは「カバレッジ最小化」または「時間最小化」のどちらかに収束しやすい
2. 報酬の重み調整（time_weight/coverage_weight）だけでは中間バランスに到達困難
3. **カバレッジペナルティの上限設定**が有望な解決策

### 8.2 次のステップ
1. **RewardDesignerにcoverage_penalty_maxパラメータを実装**
2. **v9a/v9b/v9cのquickテストを実施**し、上限値の最適値を探索
3. 有望な結果が出たら3000epで本格学習

### 8.3 v9シリーズの判断基準
| 結果 | 判断 |
|------|------|
| 直近隊選択率 40-60% + 全体RT < 10分 | **成功** → 3000epへ |
| 直近隊選択率 30-70% + 全体RT < 12分 | 有望 → パラメータ微調整 |
| 直近隊選択率が変化なし | 別アプローチ検討 |

### 8.4 学術的貢献（暫定）
- PPOを用いた救急車配車最適化における**報酬設計の課題**を特定
- カバレッジペナルティの上限設定による**収束問題の解決策**を提案
- 強化学習における「報酬の飽和」の重要性を示す事例
