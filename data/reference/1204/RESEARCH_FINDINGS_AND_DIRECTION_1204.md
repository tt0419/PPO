# PPO救急車配車最適化研究：現状の知見と今後の方針

## 概要

本ドキュメントは、PPO強化学習による救急車配車最適化研究において、これまでの実験から得られた知見と今後の方針をまとめたものである。

---

## 第1章：ベースライン運用の特性分析

### 1.1 直近隊運用の線形モデル

直近隊運用（最寄りの救急車を配車する戦略）の性能は、搬送件数と高い相関を示す。

#### 重症系応答時間モデル

```
重症系RT = 0.00917 × 件数/日 - 6.29

R² = 0.974（非常に高い説明力）
p値 = 0.0018（統計的に有意）
```

**解釈：**
- 件数が100件/日増えると、重症系RTが約0.92分増加
- 件数のみで重症系RTの97.4%を説明可能

#### 予測値と実測値の比較

| シナリオ | 件数/日 | 実測RT | 予測RT | 誤差 |
|----------|---------|--------|--------|------|
| 春季閑散期 | 1,464 | 7.08分 | 7.13分 | -0.05分 |
| 冬季閑散期 | 1,517 | 7.92分 | 7.62分 | +0.30分 |
| 冬季繁忙期 | 1,624 | 8.30分 | 8.60分 | -0.30分 |
| 夏季繁忙期 | 1,772 | 9.81分 | 9.96分 | -0.15分 |
| 勝利テスト | 1,850 | 10.87分 | 10.67分 | +0.20分 |

**→ 冬季閑散期は予測より悪い（+0.30分）：季節要因（路面凍結等）の影響が示唆される**

---

### 1.2 傷病度考慮運用の効果分析

傷病度考慮運用（重症系に優先配車する戦略）の効果は、搬送件数に強く依存する。

#### 6分達成率改善モデル

```
6分達成率改善(%) = -0.0323 × 件数/日 + 51.96

R² = 0.994（極めて高い説明力）
```

**解釈：**
- 件数が100件/日増えると、6分達成率の改善が3.23%減少
- **約1,608件/日を超えると傷病度考慮の効果がなくなる（閾値）**

#### 効果の実測値

| シナリオ | 件数/日 | 重症割合 | RT改善 | 6分改善 | 効果判定 |
|----------|---------|----------|--------|---------|----------|
| 春季閑散期 | 1,464 | 7.05% | -0.28分 | **+4.3%** | ★★★ 最も効果的 |
| 冬季閑散期 | 1,517 | 7.94% | -0.12分 | **+3.2%** | ★★☆ 効果あり |
| 冬季繁忙期 | 1,624 | 7.17% | +0.17分 | -0.2% | ★☆☆ 効果なし |
| 夏季繁忙期 | 1,772 | 5.39% | -0.04分 | **-5.5%** | ☆☆☆ 逆効果 |

#### 傷病度考慮が有効な条件

```
有効条件：
  ・件数 ≦ 約1,600件/日
  ・重症割合 ≧ 7%程度

→ 「閑散期」で効果を発揮
→ 「繁忙期」では逆効果になる可能性
```

---

## 第2章：PPO実験結果の分析

### 2.1 4シナリオ実験の結果

4つの異なる期間（冬季閑散期、春季閑散期、夏季繁忙期、冬季繁忙期）でPPOモデルを学習し、各シナリオでテストした結果。

#### 主要な発見

**発見1：PPOは全16ケースで直近隊に敗北**

| テストシナリオ | 最良PPO vs 直近隊（重症系RT） | 最良PPO vs 直近隊（6分達成率） |
|----------------|-------------------------------|-------------------------------|
| 冬季閑散期 | +0.11分 | -0.8% |
| 春季閑散期 | +0.17分 | -0.5% |
| 夏季繁忙期 | +0.45分 | -10.2% |
| 冬季繁忙期 | +0.53分 | -1.2% |

**発見2：当該シナリオで学習したモデルが最良とは限らない**

| テストシナリオ | 最良のPPOモデル | 当該モデルの順位 |
|----------------|-----------------|------------------|
| 冬季閑散期 | 冬季閑散期モデル | 1位（RT）/ 2位（6分） |
| 春季閑散期 | 冬季繁忙期モデル | 当該は2位 |
| 夏季繁忙期 | **冬季閑散期モデル** | 当該は2位 |
| 冬季繁忙期 | **冬季閑散期モデル** | 当該は2位 |

**→ 冬季閑散期モデルが繁忙期テストで最良**

---

### 2.2 勝利モデルの分析

唯一直近隊を上回ったモデル（ppo_20251117_131844）の特性を分析。

#### 勝利モデルの設定

```yaml
learning_rate:
  actor: 0.0003   # 低い学習率
  critic: 0.001

reward_weights:
  response_time: 0.5
  coverage: 0.4   # 傷病度考慮運用に近い
  workload_balance: 0.1

severity:
  reward_weight: 1.0  # 全て均等（critical, mild, moderate）

ppo:
  n_episodes: 8000

train_periods:
  - start_date: '20230115'  # 重症割合8.49%
    end_date: '20230121'
```

#### 勝利モデルの結果（テスト: 2024/12/15-12/21）

| 指標 | 直近隊 | 勝利モデル | 差 |
|------|--------|------------|-----|
| 重症系RT | 10.87分 | 10.70分 | **-0.17分** |
| 6分達成率 | 20.8% | 21.3% | **+0.5%** |

---

### 2.3 失敗した融合モデルの分析

勝利モデルと4モデルの設定を「融合」した結果、性能が悪化した。

#### 融合モデルの設定（問題点）

```yaml
learning_rate:
  actor: 0.003   # ← 4モデルから採用（高い）★問題
  critic: 0.01

# その他は勝利モデルと同じ
```

#### 結果比較

| モデル | 学習率 | 重症系RT | 6分達成率 |
|--------|--------|----------|-----------|
| 冬季閑散期（4モデル） | 0.003/0.01 | 8.03分 | 30.8% |
| 融合モデル | 0.003/0.01 | 8.43分 | 27.3% ← **悪化** |
| 勝利モデル | 0.0003/0.001 | 10.70分 | 21.3% ← **直近隊に勝利** |

**→ 高い学習率が問題である可能性**

---

## 第3章：重症割合と学習結果の関係

### 3.1 学習期間の重症割合と結果の相関

| モデル | 学習期間の重症割合 | 4シナリオでの成績 |
|--------|-------------------|-------------------|
| 勝利モデル | **8.49%**（最高） | 直近隊に勝利 |
| 冬季閑散期 | 8.49-8.78%（高） | 4モデル中最良 |
| 春季閑散期 | 6.96-7.48%（中） | 中程度 |
| 冬季繁忙期 | 7.08-7.25%（中） | 中程度 |
| 夏季繁忙期 | **5.26-5.47%**（最低） | 最も悪い |

**→ 学習期間の重症割合と結果に明確な相関がある**

### 3.2 なぜ高い重症割合で学習すると良いのか（仮説）

**仮説A：経験量の問題**
```
重症割合8.5%で8000エピソード: 約952,000件の重症系を経験
重症割合5.5%で8000エピソード: 約768,000件の重症系を経験

→ 冬季学習は重症系の経験が1.24倍多い
```

**仮説B：報酬シグナルの問題**
```
重症系は報酬/ペナルティが大きい

重症割合が高い → 強い報酬シグナルが多い → 学習の方向性が明確
重症割合が低い → 軽症系の弱い報酬が支配的 → 学習の方向性がぼやける
```

**仮説C：リソース逼迫度との関係**
```
重症割合が高い週 = 件数は少ない（閑散期）
  → 余裕がある状態で「重症系対応の判断」を純粋に学習

重症割合が低い週 = 件数は多い（繁忙期）
  → リソース逼迫で判断が歪む
```

### 3.3 2023年 重症割合Top10週（学習期間として推奨）

| 順位 | 期間 | 重症割合 | 件数/日 | 推奨度 |
|------|------|----------|---------|--------|
| 1位 | 2023/1/22-1/28 | **9.27%** | 1,352 | ★★★★★ |
| 2位 | 2023/1/29-2/4 | **8.78%** | 1,377 | ★★★★★ |
| 3位 | 2023/1/8-1/14 | **8.71%** | 1,422 | ★★★★☆ |
| 4位 | 2023/1/15-1/21 | **8.49%** | 1,352 | ★★★★☆（勝利モデル）|
| 5位 | 2023/1/1-1/7 | **8.43%** | 1,502 | ★★★★☆ |

---

## 第4章：パラメータ設定の知見

### 4.1 影響度の優先順位

| 順位 | 要因 | 影響度 | 役割 |
|------|------|--------|------|
| 1 | **学習データの重症割合** | ★★★★★ | 「何を」学習するか |
| 2 | **学習率** | ★★★★☆ | 「どのように」学習するか |
| 3 | 報酬設計 | ★★★☆☆ | 学習の方向づけ |
| 4 | エピソード数 | ★★★☆☆ | 十分な学習時間 |
| 5 | カバレッジ重み等 | ★★☆☆☆ | 微調整 |

### 4.2 推奨設定

#### 学習データ（最重要）

```
推奨：重症割合8%以上の週
  - 2023年1月の週がベスト（8.4-9.3%）
  
避けるべき：夏季（7-8月）
  - 重症割合が最も低い（5.2-6.0%）
```

#### 学習率

```
推奨：
  actor: 0.0003
  critic: 0.001

避けるべき：
  actor: 0.003（10倍高い）
  critic: 0.01
```

#### 報酬設計

```
推奨：
  response_time: 0.5
  coverage: 0.4（傷病度考慮運用に近い）
  workload_balance: 0.1

  severity reward_weight: 1.0（全て均等）
```

#### エピソード数

```
推奨：8000以上
  - 低い学習率では十分な学習時間が必要
  - 4000では収束不足の可能性
```

---

## 第5章：今後の方針

### 5.1 最優先実験：高重症週 + 低学習率

**目的：** 重症割合の効果を最大化

```yaml
experiment:
  name: "high_severity_low_lr"

train_periods:
  - start_date: '20230122'  # 重症割合9.27%（2023年1位）
    end_date: '20230128'

learning_rate:
  actor: 0.0003  # 勝利モデルと同じ
  critic: 0.001

reward_weights:
  response_time: 0.5
  coverage: 0.4
  workload_balance: 0.1

ppo:
  n_episodes: 8000
```

**期待：** 勝利モデル（8.49%）より高い9.27%で学習 → さらに良い結果

### 5.2 比較実験：要因の分離

学習率と重症割合のどちらがより重要かを検証。

```
実験A: 高重症週(9.27%) + 低学習率(0.0003) ← 最優先
実験B: 高重症週(9.27%) + 高学習率(0.003)
実験C: 低重症週(5.5%)  + 低学習率(0.0003)

比較：
  A vs B → 学習率の効果を分離
  A vs C → 重症割合の効果を分離
```

### 5.3 テストシナリオの選定

**最も勝てる可能性が高いシナリオ：**

| 優先度 | シナリオ | 理由 |
|--------|----------|------|
| 1位 | 春季閑散期 | 傷病度考慮が最も効果的（+4.3%） |
| 2位 | 冬季閑散期 | 傷病度考慮が効果あり（+3.2%） |

**避けるべきシナリオ：**
- 夏季繁忙期：傷病度考慮が逆効果（-5.5%）、件数が多すぎる

---

## 第6章：結論

### 6.1 現状のまとめ

1. **直近隊運用の性能は件数で97.4%説明可能**
   - 件数100件増加 → 重症系RT +0.92分

2. **傷病度考慮運用の効果は件数に強く依存（R²=0.994）**
   - 約1,608件/日を超えると効果がなくなる
   - 閑散期（<1,600件/日）で有効、繁忙期で逆効果

3. **PPOは全16ケースで直近隊に敗北**
   - ただし、勝利モデル（1件）は直近隊を上回った

4. **学習期間の重症割合が結果に強く影響**
   - 高重症週（8%以上）で学習 → 良い結果
   - 低重症週（5-6%）で学習 → 悪い結果

5. **学習率は低い方が安定**
   - 0.0003/0.001（勝利モデル）vs 0.003/0.01（失敗モデル）

### 6.2 今後の焦点

```
「高重症週」+「低学習率」+「閑散期テスト」

この組み合わせで直近隊を上回ることを目指す
```

---

## 付録：線形モデルまとめ

### A. 直近隊運用

```
重症系RT = 0.00917 × 件数/日 - 6.29    (R² = 0.974)
6分達成率 = -0.0306 × 件数/日 + 80.72   (R² = 0.664)
```

### B. 傷病度考慮運用の改善効果

```
6分達成率改善 = -0.0323 × 件数/日 + 51.96   (R² = 0.994)
6分達成率改善 = 3.54 × 重症割合 - 23.95     (R² = 0.745)

効果がなくなる閾値：約1,608件/日
```

---

*作成日: 2024年12月4日*
*研究：PPO強化学習による救急車配車最適化*
