# PPO救急車配車最適化研究：限界の特定と今後の方向性

**更新日: 2024年12月4日**
**ステータス: コンフィグ調整によるPPO改善の限界に到達**

---

## 第1章：実験結果サマリー

### 1.1 新モデル（ppo_20251204_133213）の結果

**学習設定:**
- Training: 2023/1/22-1/28（重症率9.27%、2023年最高）
- Validation: 2023/1/29-2/4
- Episodes: 8000
- Learning rate: 0.0003/0.001（勝利モデルと同じ）

**テスト結果:**

| 期間 | Closest | Severity-based | PPO | Winner |
|------|---------|----------------|-----|--------|
| 1/22-28 (Training) | 38.0% | **42.7%** | 35.4% | Severity |
| 2/4-10 (High Sev%) | **34.7%** | 33.9% | 32.6% | Closest |
| 12/15-21 (=1117) | 20.8% | **24.1%** | 21.1% | Severity |
| 12/22-28 (Max Cases) | 22.6% | **25.1%** | 21.7% | Severity |

### 1.2 勝利モデル（1117）との比較

| Model | Test Period | Severe RT | 6min Rate | vs Closest |
|-------|-------------|-----------|-----------|------------|
| 1117 (Winner) | 12/15-21 | 10.70 min | 21.3% | **勝利** (RT -0.17, 6min +0.5%) |
| 1204 (New) | 12/15-21 | 10.80 min | 21.1% | 僅差勝利 (RT -0.07, 6min +0.3%) |

**→ 重症率を上げても改善せず、むしろ悪化**

---

## 第2章：PPOの限界

### 2.1 観察された事実

```
事実1: PPOは傷病度考慮運用を超えられない
  - 全テスト期間で傷病度考慮 > PPO
  - Training期間でも: 傷病度42.7% >> PPO 35.4%

事実2: PPOは直近隊を一貫して下回る（一部例外）
  - Training期間: Closest 38.0% > PPO 35.4%
  - 12/15-21のみ僅差でPPO勝利

事実3: パラメータ調整では改善しない
  - 学習率: 高→不安定、低→変化なし
  - 重症率の高い週: 勝利モデルより悪化
  - エピソード数: 8000で十分、それ以上は過学習

事実4: 最も驚くべきこと
  - Training期間（最も有利なはず）でも傷病度考慮に大差で敗北
  - PPOは傷病度考慮の戦略を「学習できていない」
```

### 2.2 なぜPPOは傷病度考慮を学習できないか

**仮説1: 状態空間の問題**
```
傷病度考慮運用が使う情報:
  - 傷病度（重症/中等症/軽症）
  - カバレッジ計算（エリアごとの救急車配置）
  - 明示的なルール（重症→最寄り、軽症→カバレッジ考慮）

PPOが見る状態:
  - 傷病度情報は含まれている（はず）
  - しかし、カバレッジ計算が十分に状態に反映されていない可能性
  - 傷病度とカバレッジの「関係性」を学習できていない
```

**仮説2: 報酬の遅延問題**
```
傷病度考慮運用の判断:
  「この軽症に遠い車を出せば、カバレッジが維持される」
  → 即時に判断可能

PPOの報酬:
  配車決定 → 出動 → 現着 → 報酬
  → 「カバレッジを維持した」という即時報酬がない
  → 因果関係を学習しにくい
```

**仮説3: 探索空間の問題**
```
傷病度考慮運用:
  IF 重症 THEN 最寄り
  IF 軽症 THEN カバレッジ考慮で選択
  → 明確なルールで最適に近い解を実現

PPO:
  ランダム探索 → 報酬に基づく学習
  → 傷病度考慮のルールを「発見」するには膨大な探索が必要
  → 8000エピソードでは不足
```

**仮説4: 本質的な限界**
```
この問題の特性:
  - 最適解（傷病度考慮）が既に人間によって設計されている
  - ルールベースで十分に解ける問題
  - PPOが「人間が思いつかない戦略」を発見する余地が少ない

深層強化学習が有効な問題:
  - 状態空間が非常に大きい（囲碁、Atari）
  - 最適解が人間には分からない
  - 試行錯誤で新しい戦略を発見する価値がある

今回の問題:
  - 状態空間は比較的小さい
  - 最適解（傷病度考慮）は既知
  - PPOで新しい戦略を発見する余地が少ない
```

---

## 第3章：これまでに試したこと

### 3.1 パラメータ調整

| 試行 | 設定 | 結果 |
|------|------|------|
| 学習率 高 | 0.003/0.01 | 不安定、16戦全敗 |
| 学習率 低 | 0.0003/0.001 | 安定、僅差で勝利（1117） |
| エピソード数 | 4000→8000 | 8000で十分 |
| Coverage重み | 0.2→0.4 | 傷病度考慮に近づくが効果限定 |
| 傷病度重み | 均等(1.0) vs 傾斜 | 均等が良い |

### 3.2 学習データの選択

| 試行 | 学習期間 | 重症率 | 結果 |
|------|----------|--------|------|
| 勝利モデル | 1/15-21 | 8.49% | 直近隊に勝利 |
| 新モデル | 1/22-28 | 9.27% | 勝利モデルより悪化 |
| 春季閑散期 | 3/26-4/15 | 6.96-7.48% | 16戦全敗 |
| 夏季繁忙期 | 7/9-29 | 5.26-5.47% | 最も悪い |
| 冬季繁忙期 | 12/3-23 | 7.08-7.25% | 16戦全敗 |

### 3.3 結論

```
パラメータ調整と学習データ選択の限界に到達

- 最適な設定（勝利モデル）を見つけた
- しかし、傷病度考慮運用には到達できない
- これ以上のパラメータ調整では改善は見込めない
```

---

## 第4章：今後の選択肢

### Option A: PPOの限界を認め、研究を結論づける

**研究成果:**
1. PPOは直近隊運用を上回る条件を特定
   - 重症率7%以上の期間
   - 適切な学習率（0.0003/0.001）
   - カバレッジ重み0.4

2. しかし、傷病度考慮運用には及ばない
   - 全テスト期間で傷病度考慮 > PPO
   - パラメータ調整では改善不可

3. 実用上の推奨
   - 傷病度考慮運用を採用すべき
   - PPOは研究的価値のみ

**メリット:**
- 明確な結論が出せる
- 追加実装なしで論文化可能

**デメリット:**
- 「PPOは使えない」というネガティブな結論
- 修士論文としての貢献が限定的？

---

### Option B: PPOのアーキテクチャを根本的に見直す

**具体的な改善案:**

**B-1: 状態空間の拡張**
```python
# 現状の状態（推定）
state = [
    ambulance_positions,  # 救急車の位置
    call_location,        # 要請地点
    severity,             # 傷病度
    ...
]

# 改善案：カバレッジ情報を明示的に追加
state = [
    ambulance_positions,
    call_location,
    severity,
    coverage_score,           # 現在のカバレッジスコア
    coverage_after_dispatch,  # 配車後のカバレッジ予測
    area_coverage_map,        # エリアごとのカバレッジ
]
```

**B-2: 報酬設計の見直し**
```python
# 現状の報酬（推定）
reward = -response_time + severity_bonus

# 改善案：即時報酬の追加
reward = (
    -response_time 
    + severity_bonus
    + coverage_maintenance_bonus  # カバレッジ維持ボーナス
    + dispatch_efficiency_bonus   # 効率的な配車ボーナス
)
```

**B-3: 模倣学習の導入**
```python
# 傷病度考慮運用の判断を教師データとして使用
expert_data = collect_severity_based_decisions()
ppo.pretrain_from_demonstrations(expert_data)
ppo.fine_tune_with_rl()
```

**メリット:**
- 傷病度考慮を超える可能性
- 研究としての深みが増す

**デメリット:**
- 実装コストが高い
- 時間がかかる
- 成功するかは不確実

---

### Option C: ハイブリッドアプローチ

**具体的な方法:**
```
基本: 傷病度考慮運用をベースとする
PPO: 特定の条件でのみ判断を任せる

例:
  IF severity == 重症:
      dispatch = 傷病度考慮運用（最寄り）
  ELIF severity == 軽症 AND coverage_risk:
      dispatch = PPO判断
  ELSE:
      dispatch = 傷病度考慮運用
```

**メリット:**
- 傷病度考慮の良さを維持
- PPOで補完できる可能性

**デメリット:**
- 本質的にはPPOの貢献が限定的
- ハイブリッドの設計が難しい

---

### Option D: 問題設定の見直し

**別の研究課題への転換:**
```
現在の問題: 「配車時にどの救急車を選ぶか」
  → 傷病度考慮で十分に解ける

別の問題案:
  1. 救急車の待機位置最適化（PPOで最適な待機位置を学習）
  2. 需要予測に基づく事前配置（PPOで動的に配置を調整）
  3. 複数の同時要請への対応（複雑な割り当て問題）
```

**メリット:**
- PPOがより活きる問題設定
- 新しい研究貢献

**デメリット:**
- これまでの実験が活かせない
- 時間的に厳しい？

---

## 第5章：推奨する方向性

### 5.1 現実的な推奨（時間制約を考慮）

```
推奨: Option A + 部分的なOption B

Phase 1: 現状の結論をまとめる
  - PPOの限界を明確に文書化
  - 傷病度考慮運用との比較を論文の主軸に
  - 「いつPPOが有効か」の条件を明示

Phase 2: 可能であれば改善を試みる
  - 状態空間にカバレッジ情報を追加（比較的容易）
  - 効果があれば追加実験
  - なければPhase 1の結論を維持
```

### 5.2 論文構成案

```
1. 序論
   - 救急車配車問題の重要性
   - 強化学習の適用可能性

2. 関連研究
   - 救急車配車の最適化手法
   - PPOの救急サービスへの応用

3. 提案手法
   - PPOベースの配車最適化
   - 比較対象（直近隊、傷病度考慮）

4. 実験
   - 4シナリオ×4モデルの網羅的評価
   - パラメータ感度分析
   - 重症率と効果の関係

5. 考察
   - PPOが有効な条件の特定
   - PPOの限界と原因分析
   - 傷病度考慮運用との比較

6. 結論
   - PPOは直近隊を上回る条件が存在する
   - しかし、傷病度考慮運用が最も効果的
   - 今後の研究方向性
```

### 5.3 研究としての貢献

```
貢献1: PPOの適用条件の明確化
  - 重症率7%以上で効果がある
  - 学習データの選択が重要
  - パラメータ設定のガイドライン

貢献2: 傷病度考慮運用の有効性の定量的評価
  - 閑散期で+3〜4%の改善
  - 繁忙期では効果なし〜逆効果
  - 件数ではなく重症率が決定要因

貢献3: 強化学習の限界の明示
  - ルールベースで解ける問題には不向き
  - 状態空間設計の重要性
  - 報酬設計の難しさ
```

---

## 結論

**現状の評価:**
```
傷病度考慮運用 >> PPO ≈ 直近隊運用

PPOは直近隊を「僅かに」上回ることは可能だが、
傷病度考慮運用には到達できない。

コンフィグ調整とデータ選択の限界に達した。
```

**推奨アクション:**
```
1. 現状の結果を論文としてまとめる
2. PPOの限界を「研究成果」として位置づける
3. 時間があれば状態空間の改善を試みる
4. 傷病度考慮運用の実用化を推奨
```

---

*作成日: 2024年12月4日*
*ステータス: PPOパラメータ調整の限界に到達*
